% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}
Large Language Models (LLMs) have become essential across various applications due to their exceptional performance. As model performance tends to improve with increased parameter counts, LLMs have been significantly scaled in recent years, with contemporary models now reaching 500B+ parameters \parencite{palm}.

Deploying such large models for inference presents major challenges \parencite{efficientlyscaling}. Tensor Parallel \parencite{megatron} addresses this by splitting layers on multiple accelerators, enabling the execution of extremely large models and significantly reducing latency. However, Tensor Parallel demands accumulation of results from accelerators, as shown in figure \ref{fig:overview}, and can lead to data communication bottlenecks \parencite{optimizingcommunication, mnemosyne}, especially during the first auto-regressive inference step (the prefill phase).

One approach to mitigate these bottlenecks, and thus reduce model latency even further, is to quantize activations before communication, which reduces the time needed to accumulate results from accelerators in a Tensor Parallel group. However, the presence of outliers \parencite{llmint8, awq} complicates this strategy, necessitating fine-grained quantization approaches. We leverage such approaches proposed by \cite{microscaling} to compress activations and demonstrate the potency of communication compression by measuring time-to-first-token (TTFT) in realistic inference scenarios using different inference hardware setups. We find that for hardware setups which have slower inter-accelerator bandwidths, the TTFT can be improved by 3.5 - 4.5x with negligible degradation of model performance.
